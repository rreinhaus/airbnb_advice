{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6c5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Loading the listings data set\n",
    "\n",
    "listings_data = pd.read_csv('/home/rreinhaus/code/rreinhaus/listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21051d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Holiday London DB Room Let-on going'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee61c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_data = listings_data[['id', 'description']][listings_data['review_scores_rating'] > 4.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2f12d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My bright double bedroom with a large window has a relaxed feeling! It comfortably fits one or two and is centrally located just two blocks from Finsbury Park. Enjoy great restaurants in the area and easy access to easy transport tubes, trains and buses. Babies and children of all ages are welcome.<br /><br /><b>The space</b><br />Hello Everyone,<br /><br />I'm offering my lovely double bedroom in Finsbury Park area (zone 2) for let in a shared apartment. <br />You will share the apartment with me and it is fully furnished with a self catering kitchen. Two people can easily sleep well as the room has a queen size bed. I also have a travel cot for a baby for guest with small children. <br /><br />I will require a deposit up front as a security gesture on both our parts and will be given back to you when you return the keys. <br /><br />I trust anyone who will be responding to this add would treat my home with care and respect . <br /><br />Best Wishes <br /><br />Alina<br /><br /><b>Gue\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_data['description'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc05aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n"
     ]
    }
   ],
   "source": [
    "word_list = description_data['description'][0].split()\n",
    "\n",
    "number_of_words = len(word_list)\n",
    "\n",
    "print(number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6754e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku\n",
    "import tensorflow as tf\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e26f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations and convert text to lowercase\n",
    "def clean_text(text):\n",
    "    text = ''.join(e for e in text if e not in string.punctuation).lower()\n",
    "    \n",
    "    text = text.encode('utf8').decode('ascii', 'ignore')\n",
    "    return text\n",
    "\n",
    "corpus = [clean_text(str(e)) for e in description_data['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "effe13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    #get tokens\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    #convert to sequence of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "806aa78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8c0607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences,  maxlen=max_sequence_len, padding='pre'))\n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:, -1]\n",
    "    label = ku.to_categorical(label, num_classes = total_words)\n",
    "    \n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ccbaada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b34e88ee0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 â€” LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.fit(predictors, label, epochs=20, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "540a2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1,  padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        \n",
    "        for word ,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += ' ' +output_word\n",
    "    \n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7c5af1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Room The The The The The\n"
     ]
    }
   ],
   "source": [
    "print(generate_text('Room', 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2557d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
